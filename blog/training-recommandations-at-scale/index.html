<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Recommendation Models at Scale</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  <!-- Slick Carousel -->
  <link rel="stylesheet" href="https://trog.me/plugins/slick/slick.css" />
  <link rel="stylesheet" href="https://trog.me/plugins/slick/slick-theme.css" />
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://trog.me/plugins/font-awesome/css/font-awesome.min.css" />

  <!-- Magnific Popup -->
  <link rel="stylesheet" href="https://trog.me/plugins/magnafic-popup/magnific-popup.css" />

  <!-- Stylesheets -->
  
  <link href="https://trog.me/scss/style.min.css" rel="stylesheet" />

  <!--Favicon-->
  <link rel="shortcut icon" href="https://trog.me/images/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="https://trog.me/images/favicon.png" type="image/x-icon" />
  
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-G3GCFC8MQ8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-G3GCFC8MQ8');
  </script>
  
</head>

<body>
  <nav class="navbar navbar-expand-lg fixed-top">
  <div class="container">
    <a href="https://trog.me/" class="navbar-brand">
      <img src="https://trog.me/images/site-navigation/icons8-visualization-skill-64.png" alt="site-logo">
    </a>
    <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#navbarCollapse">
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
      <ul class="nav navbar-nav main-navigation my-0 mx-auto">
        
        
        <li class="nav-item">
          <a href="https://trog.me/#home"
            class="nav-link text-dark text-sm-center p-2 ">Home</a>
        </li>
        
        <li class="nav-item">
          <a href="https://trog.me/#about"
            class="nav-link text-dark text-sm-center p-2 ">About</a>
        </li>
        
        <li class="nav-item">
          <a href="https://trog.me/#resume"
            class="nav-link text-dark text-sm-center p-2 ">Resume</a>
        </li>
        
        <li class="nav-item">
          <a href="https://trog.me/#why-would-you-work-with-me-"
            class="nav-link text-dark text-sm-center p-2 ">Skills</a>
        </li>
        
        <li class="nav-item">
          <a href="https://trog.me/#blog"
            class="nav-link text-dark text-sm-center p-2 ">Blog</a>
        </li>
        
      </ul>
      <div class="navbar-nav">
        <a href="https://trog.me/contact" class="btn btn-primary btn-zoom hire_button">Hire Me Now</a>
      </div>
    </div>
  </div>
</nav>
  <div id="content">
    

<header class="breadCrumb">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
        <h3 class="breadCrumb__title">Recommendation Models at Scale</h3>
        <nav aria-label="breadcrumb" class="d-flex justify-content-center">
          <ol class="breadcrumb align-items-center">
            <li class="breadcrumb-item"><a href=https://trog.me/>Home</a></li>
            <li class="breadcrumb-item"><a href=https://trog.me/blog>All Post</a></li>
            <li class="breadcrumb-item active" aria-current="page">Recommendation Models at Scale</li>
          </ol>
        </nav>
      </div>
    </div>
  </div>
</header>

<section class="section singleBlog">
  <div class="svg-img">
      <img src=https://trog.me/images/hero/figure-svg.svg alt="">
  </div>
  <div class="animate-shape">
      
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600">
          <defs>
              <linearGradient id="d" x1="0.929" y1="0.111" x2="0.263" y2="0.935" gradientUnits="objectBoundingBox">
                  <stop offset="0" stop-color="#f1f6f9" />
                  <stop offset="1" stop-color="#f1f6f9" stop-opacity="0" />
              </linearGradient>
          </defs>
          <g data-name="blob-shape (3)">
              <path class="blob" fill="url(#d)"
                  d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z" />
          </g>
      </svg>
  </div>
  <div class="animate-pattern">
      <img src=https://trog.me/images/service/background-pattern.svg alt="background-shape">
  </div>
  <div class="container">
      <div class="row">
          <div class="col-lg-12">
              <div class="singleBlog__feature">
                  <img src=https://trog.me/images/blog/dam.jpg alt="feature-image">
              </div>
          </div>
      </div>
      <div class="row mt-5">
          <div class="col-lg-12">
              <div class="singleBlog__content">
                  <p>Today we&rsquo;ll talk about FAIRS&rsquo;s paper: <a href="https://arxiv.org/pdf/2011.05497.pdf">Understanding Training Efficiency of Deep Learning Recommendation Models at Scale</a>.</p>
<hr>
<h1 id="overview">Overview</h1>
<p>In this paper, Facebook researchers experiment on and analyze the training stage of large recommender systems. This goes from hardware consideration to embedding strategies. From data representation considerations to task distribution policy. Their goal is to be able to provide a more in-depth understanding of what can affect the speed and accuracy of a large production recommendation model. Namely, they compare the use of three different hardware set-ups in use at Facebook, to train three of their large-scale recommender systems. They chose those specific three models to compare as their specificities are quite different from one to another and allow one to better understand what is to be taken into consideration before defining a deployment policy.</p>
<h1 id="facebook-ml-training">Facebook ML training</h1>
<p>Before we jump into the experiments, let us first have a look at some facts on ML training. Facebook uses a custom infrastructure for their model training and use several types of machines for that purpose.</p>
<h3 id="generic-background">Generic background</h3>
<p>Facebook routinely trains a high number of models for its different services. They all have different training times and frequencies depending on the specific requirements. It is therefore important that training times are kept low to reduce resource utilization in the long run, as well as avoiding the deployments of already stale models may they take too long to train.</p>
<!-- raw HTML omitted -->
<p><img src="https://trog.me/images/blog/Fb_ML_usecases.png" alt="Training time over frequency"></p>
<p>In our case, the authors describe three major steps in the training of a recommendation model at Facebook.</p>
<ol>
<li>Data fetching and pre-processing</li>
<li>Actual training (we will focus on this one)</li>
<li>Model deployment</li>
</ol>
<p>To tackle the challenge of performing that many training runs, Facebook developed its GPU-based architecture called Big Basin. This architecture was not tailored to recommendation model handling. Therefore, their hardware requires extra care for an efficient strategy for the serving of embedding tables in the context of recommender systems.</p>
<h3 id="recommendation-model-specificities">Recommendation model specificities</h3>
<p>To understand what may affect the training performances of those models, the authors first summarize their two main components. Embedding layers used to represent categorical features by dense vectors and multi-layer perceptrons used to represent continuous features. Both components are used to learn latent space representation of their respective features which are then used by the rest of the model. For the MLP components, each trainer holds a copy of the model during training and regularly updates it with the rest of the workers. The problem with the embeddings table can&rsquo;t be solved this way. As there is a potentially very high number of embeddings to store in memory, it is not possible to fit them all into a single machine. There is therefore a partition of those embeddings across different embedding servers. Those servers store only a fraction of all embeddings and thus are called sparse parameter servers.</p>
<p>When looking at training performance, one might look at resource usage. In our case CPU, memory, and network usage. It happens that these usages resemble a gaussian distribution with its parameters depending on the model&rsquo;s characteristics and configuration. In the figure below, we see the distribution of resource utilization of multiple train runs of the same model in the same environment but with different parameters. The authors suggest that the variability in the distributions comes from these differences in parameters as well as some system-level variability. That means that while not being totally predictable, model training runs behave consistently in their resource utilization. Therefore, it makes sense to look for the most suitable hardware when deploying those ML models to production.</p>
<!-- raw HTML omitted -->
<p><img src="https://trog.me/images/blog/trainer_util.png" alt="Resource utilization distribution"></p>
<h3 id="hardware-considerations">Hardware considerations</h3>
<p>In the following experiments, three distinct hardware setups will be used.</p>
<ol>
<li>The first one is a two CPU machine and is called the <strong>dual-socket</strong>.</li>
<li>The second is composed of 2 CPUs and 8 V100 GPUs interconnected using NVLINK and is called <strong>Big Basin</strong> and offers about 900 GB/s of bandwidth.</li>
<li>The third machine is made of 8 CPUs and 8 GPUs and offers a whopping 2 TB/s of bandwidth and is called <strong>Zion</strong>.</li>
</ol>
<p>Each of those setups requires a custom configuration of the software running it. Therefore each of them has to get their configuration tuned before being able to properly compare them. Examples of these parameters to set are the number of working and data loading threads, embedding placement strategy, and the number of workers. We will describe here the different placement strategies explored in the paper.</p>
<p>The first and obvious strategy is to solely use the GPU memory to store embeddings. This will favor systems with lower GPU-CPU bandwidth. In the case where the table does not fit into the GPUs memory, the inter-worker communication becomes the primary bottleneck.</p>
<p>The second method is to use the CPU memory. In that case, a lot of CPU-GPU copies have to be made. Therefore, only systems with high GPU-CPU bandwidth will be able to perform well. Here, the CPUs can become a bottleneck as they will have to bear the load of all copies being made.</p>
<p>The third approach is to store the embeddings on remove CPU memory. This will tackle the bottleneck seen in the previous approach as it enables the scaling of the number of CPUs handling the operations on embeddings. However, it also increases latency as networking is involved.</p>
<p>Last but not least, a hybrid approach can also be used where some embeddings are stored on the GPU memory and some others are stored on the CPU side. In that case, the stress on CPUs is partially relieved as part of their workload is directly handled by the GPUs.</p>
<h1 id="the-efficiency-of-various-model-configurations">The efficiency of various model configurations</h1>
<p>The different architectures seen above are tested using different combinations of the number of sparse and dense feature pairs. The intuition behind it is that the lower the number of features used by the model, the higher the throughput. Let us get a feeling of their findings through the following figures:</p>
<!-- raw HTML omitted -->
<p><img src="https://trog.me/images/blog/throughput_cpu.png" alt="CPU throughput visualization"></p>
<p>The figure above represents the throughput of the model training according to both the number of sparse and dense features on CPU. We can see that past 64 sparse features, the model sees a huge hit in performance. This is probably due to the fact that the embeddings no longer fit into single host memory and increased usage of networking is required.</p>
<!-- raw HTML omitted -->
<p><img src="https://trog.me/images/blog/throughput_gpu.png" alt="GPU throughput visualization"></p>
<p>In this figure, we see that the GPU-based throughput is higher no matter what. Again we see a clear step in throughput as the number of features varies.</p>
<!-- raw HTML omitted -->
<p><img src="https://trog.me/images/blog/efficiency_table.png" alt="Efficiency ratio between GPU and CPU"></p>
<p>The authors note that on average, <strong>Big Basin</strong> consumes 7.3 times more power than the CPU system. This means that the power efficiency isn&rsquo;t necessarily better for GPU workers. In the figure above, we see that for the lowest number of features, the CPU consumes less power to train. We even see that the less dense features occur in a worse performance of the GPU relative to the one of the CPU.</p>
<p>Another interesting finding of the paper is the different relations between the hash size and throughput. Hash tables are used to limit the set of possible sparse feature values. The higher the hash size is, the more possible embedding values become possible. Of course, a large number of embeddings means a lot of memory consumption. In our case, several hash sizes are tested on both CPU and GPU architectures.</p>
<!-- raw HTML omitted -->
<p><img src="https://trog.me/images/blog/hsx_cpu.png" alt="Throughput of CPU over hash size"></p>
<p>In this first figure, the different lines correspond to different combinations of the number of sparse and dense features. we can observe that the throughput remains &ldquo;stable&rdquo; no matter what the hash size is. No clear correlation can quickly be observed here. The explanation of the authors is simple. For all experiments shown here, the host memory was enough to store the embeddings.</p>
<!-- raw HTML omitted -->
<p><img src="https://trog.me/images/blog/hsx_gpu.png" alt="Throughput of GPU over hash size"></p>
<p>In this second figure, however, things are different. We can see that as the hash size grows, the throughput does take a hit. We can even see that the higher the throughput values are, the bigger the hit is. In this case, as the hash size is growing, the number of GPU required to store the embeddings grows alongside the inter-GPU communications. This is why we see such a dramatic drop in throughput here.</p>
<h1 id="real-world-efficiency">Real world efficiency</h1>
<p>To conclude this overview, we will look at some insights collected on some real models used in production at Facebook. They all present different characteristics and we&rsquo;ll see that choosing the hardware and the memory placement strategy isn&rsquo;t straightforward at all.</p>
<!-- raw HTML omitted -->
<p><img src="https://trog.me/images/blog/case_study.png" alt="Case study"></p>
<p>This first column presents results for the M1 model. It has a low number of sparse features but a high number of dense features. Its embedding table size is in the order of tens of GB. These characteristics are perfect for a GPU system as they perform really well on dense features and when the memory consumption is limited. We see that the GPU is better in both throughput and training efficiency.</p>
<p>The second column presents the model M2, which has similar characteristics to the M1 except for the number of dense features where it is low. This lower number of dense features is enough to reduce the edge GPUs have and make CPU systems actually faster than the GPU system at this task. It is interesting to note, however, that despite being slower, the GPU system still presents a better power efficiency.</p>
<p>The last column presents the model M3. M3 has a high number of both sparse and dense features, its embedding table size is in the order of hundreds of GB which makes it way heavier than the two previous models. As we have seen previously, this memory consumption causes a lot of problems for the GPU as there have to be a lot more data copies. This considerably impacts the performance of the model on GPU systems and makes it both faster and more power-efficient to train on CPU systems.</p>

              </div>
          </div>
      </div>
  </div>
</section>


  </div>
  <section class="footer" id="contact">
	<div class="footer__background_shape">
		<svg viewBox="0 0 1920 79">
			<path d="M0 0h1920v79L0 0z" data-name="Path 1450" />
		</svg>
	</div>
	<div class="container">
		<div class="row">
			<div class="col-lg-12">
				<div class="footer__cta">
					<div class="shape-1">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="shape-2">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="text-light footer__cta_content">
						<span>Contact me</span>
						<h2 class="mb-0">Let’s Start a Project</h2>
					</div>
					<div class="footer__cta_action">
						<a class="btn btn-light btn-zoom" href="https://trog.me/contact">Get in
							touch</a>
					</div>
				</div>
			</div>
		</div>
		<div class="row footer__widget">
			<div class="col-lg-4">
				<div class="footer__widget_logo mb-5">
					<img src="https://trog.me/images/site-navigation/icons8-visualization-skill-64.png" alt="widget-logo">
				</div>
			</div>
			<div class="col-lg-4">
				<div class="text-light footer__widget_sitemap mb-5">
					<h4 class="base-font">Sitemap</h4>
					<ul class="unstyle-list small">
						
						
						<li class="mb-2"><a class="text-light" href="https://trog.me/">Home</a></li>
						
						<li class="mb-2"><a class="text-light" href="https://trog.me/blog">Blog</a></li>
						
					</ul>
				</div>
			</div>
			<div class="col-lg-4">
				<div class="text-light footer__widget_address mb-5">
					<h4 class="base-font">Address</h4>
					
					<ul class="fa-ul small">
						<li class="mb-2"><a class="text-light" href="tel:"><span class="fa-li"><i
										class="fa fa-phone"></i></span></a></li>
						<li class="mb-2"><a class="text-light" href="mailto:ilan@trog.me"><span class="fa-li"><i
										class="fa fa-envelope"></i></span>ilan@trog.me</a></li>
						<li class="mb-2">
							<span class="fa-li"><i class="fa fa-map-marker"></i></span>Berlin, Germany</a>
						</li>
					</ul>
				</div>
			</div>
		</div>
		<div class="row footer__footer">
			<div class="col-lg-6">
				<div class="footer__footer_copy text-light">
					<p>The logo is the courtesy of Icons8.</p>
				</div>
			</div>
			<div class="col-lg-6">
				<div class="footer__footer_social">
					<ul class="unstyle-list">
						
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://www.linkedin.com/in/ilandubois/"><i
									class="fa fa-linkedin-square"></i></a>
						</li>
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://github.com/trog-levrai/"><i
									class="fa fa-github-square"></i></a>
						</li>
						
					</ul>
				</div>
			</div>
		</div>
	</div>
</section>
<script src="https://maps.googleapis.com/maps/api/js?key=&libraries=geometry"></script>
<script src="https://trog.me/plugins/jQuery/jquery.min.js"></script>
<script src="https://trog.me/plugins/bootstrap/bootstrap.min.js"></script>
<script src="https://trog.me/plugins/slick/slick.min.js"></script>
<script src="https://trog.me/plugins/slick/slick.min.js"></script>
<script src="https://trog.me/plugins/waypoint/jquery.waypoints.min.js"></script>
<script src="https://trog.me/plugins/magnafic-popup/jquery.magnific-popup.min.js"></script>
<script src="https://trog.me/plugins/tweenmax/TweenMax.min.js"></script>
<script src="https://trog.me/plugins/imagesloaded/imagesloaded.min.js"></script>
<script src="https://trog.me/plugins/masonry/masonry.min.js"></script>

<script src="https://trog.me/js/form-handler.min.js"></script>

<script src="https://trog.me/js/script.min.js"></script>
</body>

</html>